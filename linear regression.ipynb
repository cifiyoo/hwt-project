{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3db96ab",
   "metadata": {},
   "source": [
    "Popular(i,u)=α×Current\\_Popular(i)+β×Trend(i,t)+ϵ×Similarity(u,others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Popular-评分-预测\n",
    "# Current_Popular - 目前电影评分，\n",
    "# Trend - 通过时间+评分计算\n",
    "# Similarity - 协同过滤的预测值\n",
    "\n",
    "# 假设 data 是你的数据集，包含了 P, X, Y, Z\n",
    "X = data[['X', 'Y', 'Z']]  # 特征矩阵\n",
    "y = data['P']  # 目标值\n",
    "\n",
    "# 创建线性回归模型\n",
    "model = LinearRegression()\n",
    "\n",
    "# 拟合模型\n",
    "model.fit(X, y)\n",
    "\n",
    "# 输出权重因子 A, B, C\n",
    "A = model.coef_[0]\n",
    "B = model.coef_[1]\n",
    "C = model.coef_[2]\n",
    "\n",
    "print(f\"A = {A}, B = {B}, C = {C}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff5f84",
   "metadata": {},
   "source": [
    "线性回归的一个限制是它只能捕捉线性关系。在许多现实世界的问题中，因变量和自变量的关系可能是非线性的。通过创建自变量的多项式特征，我们可以使用PolynomialFeatures类将非线性关系转化为线性形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=3)\n",
    "X_poly = poly_features.fit_transform(X_train)\n",
    "\n",
    "# 使用多项式特征重新训练模型\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_poly, y_train)\n",
    "\n",
    "# 预测并评估\n",
    "y_pred_poly = model_poly.predict(poly_features.transform(X_test))\n",
    "mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
    "print(f\"Mean Squared Error with Polynomials: {mse_poly}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f2e8ff",
   "metadata": {},
   "source": [
    "正则化是一种防止过拟合的技术，通过在损失函数中添加一个惩罚项来限制模型的复杂度。L1正则化（Lasso）和L2正则化（Ridge）是两种常见的方法。在Scikit-Learn中，可以使用Lasso或Ridge类实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b042fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "# 使用Lasso正则化\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "print(f\"Mean Squared Error with Lasso: {mse_lasso}\")\n",
    "\n",
    "# 使用Ridge正则化\n",
    "ridge_model = Ridge(alpha=0.1)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "print(f\"Mean Squared Error with Ridge: {mse_ridge}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed77760",
   "metadata": {},
   "source": [
    "特征选择\n",
    "\n",
    "在具有大量特征的数据集中，特征选择可以帮助减少模型复杂度，提高模型的解释性。可以使用SelectKBest类结合一个统计测试（如f_regression）来选择最相关的特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a5ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# 选择最重要的k个特征\n",
    "selector = SelectKBest(score_func=f_regression, k=2)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# 使用选定的特征训练和评估模型\n",
    "model_kbest = LinearRegression()\n",
    "model_kbest.fit(X_train_selected, y_train)\n",
    "y_pred_kbest = model_kbest.predict(X_test_selected)\n",
    "mse_kbest = mean_squared_error(y_test, y_pred_kbest)\n",
    "print(f\"Mean Squared Error with KBest Features: {mse_kbest}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef252bb",
   "metadata": {},
   "source": [
    "超参数调优\n",
    "\n",
    "使用网格搜索或随机搜索来找到最优的模型参数。GridSearchCV和RandomizedSearchCV可以帮助自动化这个过程：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be20338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# 对Ridge模型进行参数调优\n",
    "ridge_params = {'alpha': [0.1, 0.5, 1.0, 5.0, 10.0]}\n",
    "ridge_search = GridSearchCV(Ridge(), ridge_params, scoring='neg_mean_squared_error', cv=5)\n",
    "ridge_search.fit(X_train, y_train)\n",
    "best_ridge = ridge_search.best_estimator_\n",
    "y_pred_tuned = best_ridge.predict(X_test)\n",
    "mse_tuned = mean_squared_error(y_test, y_pred_tuned)\n",
    "print(f\"Mean Squared Error with Tuned Ridge: {mse_tuned}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0f593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
